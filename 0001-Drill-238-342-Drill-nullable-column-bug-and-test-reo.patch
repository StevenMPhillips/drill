From 0df3d0d5c6fd5645ad0019af952600a18e1399f1 Mon Sep 17 00:00:00 2001
From: Jason Altekruse <altekrusejason@gmial.com>
Date: Fri, 3 Jan 2014 17:35:49 -0600
Subject: [PATCH] Drill 238/342 Drill nullable column bug and test
 reorganization

---
 .../drill/exec/store/json/JSONRecordReader.java    |   2 +-
 .../exec/store/parquet/VarLenBinaryReader.java     |   4 +
 .../drill/exec/store/CachedSingleFileSystem.java   | 171 +++++
 .../drill/exec/store/ParquetRecordReaderTest.java  | 738 ---------------------
 .../apache/drill/exec/store/TestOutputMutator.java |  74 +++
 .../apache/drill/exec/store/parquet/FieldInfo.java |  40 ++
 .../store/parquet/ParquetRecordReaderTest.java     | 446 ++++++++-----
 .../exec/store/parquet/ParquetResultListener.java  | 202 ++++++
 .../exec/store/parquet/ParquetTestProperties.java  |  37 ++
 .../exec/store/parquet/TestFileGenerator.java      | 142 ++--
 .../exec/store/parquet/WrapAroundCounter.java      |  40 ++
 11 files changed, 891 insertions(+), 1005 deletions(-)
 create mode 100644 exec/java-exec/src/test/java/org/apache/drill/exec/store/CachedSingleFileSystem.java
 delete mode 100644 exec/java-exec/src/test/java/org/apache/drill/exec/store/ParquetRecordReaderTest.java
 create mode 100644 exec/java-exec/src/test/java/org/apache/drill/exec/store/TestOutputMutator.java
 create mode 100644 exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/FieldInfo.java
 create mode 100644 exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetResultListener.java
 create mode 100644 exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetTestProperties.java
 create mode 100644 exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/WrapAroundCounter.java

diff --git a/exec/java-exec/src/main/java/org/apache/drill/exec/store/json/JSONRecordReader.java b/exec/java-exec/src/main/java/org/apache/drill/exec/store/json/JSONRecordReader.java
index 11b972c..d2f27dd 100644
--- a/exec/java-exec/src/main/java/org/apache/drill/exec/store/json/JSONRecordReader.java
+++ b/exec/java-exec/src/main/java/org/apache/drill/exec/store/json/JSONRecordReader.java
@@ -215,7 +215,7 @@ public class JSONRecordReader implements RecordReader {
 
   private boolean fieldSelected(String field){
     SchemaPath sp = new SchemaPath(field, ExpressionPosition.UNKNOWN);
-    if (this.columns != null && this.columns.size() > 1){
+    if (this.columns != null && this.columns.size() > 0){
       for (SchemaPath expr : this.columns){
         if ( sp.equals(expr)){
           return true;
diff --git a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/VarLenBinaryReader.java b/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/VarLenBinaryReader.java
index 0321838..d9e498e 100644
--- a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/VarLenBinaryReader.java
+++ b/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/VarLenBinaryReader.java
@@ -103,6 +103,10 @@ public class VarLenBinaryReader {
     do {
       lengthVarFieldsInCurrentRecord = 0;
       for (ColumnReader columnReader : columns) {
+        if (recordsReadInCurrentPass == columnReader.valueVecHolder.getValueVector().getValueCapacity()){
+          rowGroupFinished = true;
+          break;
+        }
         if (columnReader.pageReadStatus.currentPage == null
             || columnReader.pageReadStatus.valuesRead == columnReader.pageReadStatus.currentPage.getValueCount()) {
           columnReader.totalValuesRead += columnReader.pageReadStatus.valuesRead;
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/CachedSingleFileSystem.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/CachedSingleFileSystem.java
new file mode 100644
index 0000000..46a2785
--- /dev/null
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/CachedSingleFileSystem.java
@@ -0,0 +1,171 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store;
+
+import io.netty.buffer.ByteBuf;
+import io.netty.buffer.ByteBufInputStream;
+import io.netty.buffer.UnpooledByteBufAllocator;
+
+import java.io.BufferedInputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URI;
+
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PositionedReadable;
+import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.util.Progressable;
+
+public class CachedSingleFileSystem extends FileSystem{
+
+  private ByteBuf file;
+  private String path;
+  
+  public CachedSingleFileSystem(String path) throws IOException{
+    this.path = path;
+    File f = new File(path);
+    long length = f.length();
+    if(length > Integer.MAX_VALUE) throw new UnsupportedOperationException("Cached file system only supports files of less than 2GB.");
+    System.out.println(length);
+    try(InputStream is = new BufferedInputStream(new FileInputStream(path))){
+      byte[] buffer = new byte[64*1024];
+      this.file = UnpooledByteBufAllocator.DEFAULT.directBuffer((int) length);
+      int read;
+      while( (read = is.read(buffer)) > 0){
+        file.writeBytes(buffer, 0, read);
+      }
+    }
+  }
+
+  public void close() throws IOException{
+    file.release();
+    super.close();
+  }
+  
+  @Override
+  public FSDataOutputStream append(Path arg0, int arg1, Progressable arg2) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public FSDataOutputStream create(Path arg0, FsPermission arg1, boolean arg2, int arg3, short arg4, long arg5,
+      Progressable arg6) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public boolean delete(Path arg0) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public boolean delete(Path arg0, boolean arg1) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public FileStatus getFileStatus(Path arg0) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public URI getUri() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public Path getWorkingDirectory() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public FileStatus[] listStatus(Path arg0) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public boolean mkdirs(Path path, FsPermission arg1) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public FSDataInputStream open(Path path, int arg1) throws IOException {
+    if(!path.toString().equals(this.path)) throw new IOException(String.format("You requested file %s but this cached single file system only has the file %s.", path.toString(), this.path));
+    return new FSDataInputStream(new CachedFSDataInputStream(file.slice()));
+  }
+
+  @Override
+  public boolean rename(Path arg0, Path arg1) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public void setWorkingDirectory(Path arg0) {
+    throw new UnsupportedOperationException();
+  }
+  
+  
+  private class CachedFSDataInputStream extends ByteBufInputStream implements Seekable, PositionedReadable{
+    private ByteBuf buf;
+    public CachedFSDataInputStream(ByteBuf buffer) {
+      super(buffer);
+      this.buf = buffer;
+      
+    }
+
+    @Override
+    public long getPos() throws IOException {
+      return buf.readerIndex();
+    }
+
+    @Override
+    public void seek(long arg0) throws IOException {
+      buf.readerIndex((int) arg0);
+    }
+
+    @Override
+    public boolean seekToNewSource(long arg0) throws IOException {
+      return false;
+    }
+
+    @Override
+    public int read(long pos, byte[] buffer, int offset, int length) throws IOException {
+      ByteBuf local = buf.slice( (int) pos, (int) Math.min( buf.capacity() - pos, length));
+      local.readBytes(buffer, offset, buf.capacity());
+      return buf.capacity();
+    }
+
+    @Override
+    public void readFully(long pos, byte[] buffer) throws IOException {
+      readFully(pos, buffer, 0, buffer.length);
+    }
+
+    @Override
+    public void readFully(long pos, byte[] buffer, int offset, int length) throws IOException {
+      if(length + pos > buf.capacity()) throw new IOException("Read was too big.");
+      read(pos, buffer, offset, length);
+    }
+  }
+}
\ No newline at end of file
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/ParquetRecordReaderTest.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/ParquetRecordReaderTest.java
deleted file mode 100644
index 6168ff6..0000000
--- a/exec/java-exec/src/test/java/org/apache/drill/exec/store/ParquetRecordReaderTest.java
+++ /dev/null
@@ -1,738 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.drill.exec.store;
-
-import com.beust.jcommander.internal.Lists;
-import com.google.common.base.Charsets;
-import com.google.common.io.Files;
-import com.google.common.util.concurrent.SettableFuture;
-
-import org.apache.drill.common.config.DrillConfig;
-import org.apache.drill.common.types.TypeProtos;
-import org.apache.drill.common.util.FileUtils;
-import org.apache.drill.exec.client.DrillClient;
-import org.apache.drill.exec.exception.SchemaChangeException;
-import org.apache.drill.exec.physical.impl.OutputMutator;
-import org.apache.drill.exec.proto.UserBitShared.QueryId;
-import org.apache.drill.exec.proto.UserProtos;
-import org.apache.drill.exec.record.MaterializedField;
-import org.apache.drill.exec.record.RecordBatchLoader;
-import org.apache.drill.exec.record.VectorWrapper;
-import org.apache.drill.exec.rpc.RpcException;
-import org.apache.drill.exec.rpc.user.QueryResultBatch;
-import org.apache.drill.exec.rpc.user.UserResultsListener;
-import org.apache.drill.exec.server.Drillbit;
-import org.apache.drill.exec.server.RemoteServiceSet;
-import org.apache.drill.exec.store.json.JsonSchemaProvider;
-import org.apache.drill.exec.vector.BaseDataValueVector;
-import org.apache.drill.exec.vector.ValueVector;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.junit.Ignore;
-import org.junit.Test;
-
-import parquet.bytes.BytesInput;
-import parquet.column.ColumnDescriptor;
-import parquet.hadoop.ParquetFileWriter;
-import parquet.hadoop.metadata.CompressionCodecName;
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-
-import java.util.*;
-
-import static org.junit.Assert.*;
-import static parquet.column.Encoding.PLAIN;
-
-public class ParquetRecordReaderTest {
-  org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ParquetRecordReaderTest.class);
-
-  private static final boolean VERBOSE_DEBUG = false;
-
-  // { 00000001, 00000010, 00000100, 00001000, 00010000, ... }
-  byte[] bitFields = {1, 2, 4, 8, 16, 32, 64, -128};
-  byte allBitsTrue = -1;
-  byte allBitsFalse = 0;
-  int DEFAULT_BYTES_PER_PAGE = 1024 * 1024 * 1;
-  static Object[] intVals = {-200, 100, Integer.MAX_VALUE };
-  static Object[] longVals = { -5000l, 5000l, Long.MAX_VALUE};
-  static Object[] floatVals = { 1.74f, Float.MAX_VALUE, Float.MIN_VALUE};
-  static Object[] doubleVals = {100.45d, Double.MAX_VALUE, Double.MIN_VALUE,};
-  static Object[] boolVals = {false, false, true};
-  static byte[] varLen1 = {50, 51, 52, 53, 54, 55, 56, 57, 58, 59};
-  static byte[] varLen2 = {15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1};
-  static byte[] varLen3 = {100, 99, 98};
-  static Object[] binVals = { varLen3, varLen2, varLen3};
-  static Object[] bin2Vals = { varLen3, varLen2, varLen1};
-
-  private void populateFieldInfoMap(ParquetTestProperties props){
-    props.fields.put("integer", new FieldInfo("int32", "integer", 32, intVals, TypeProtos.MinorType.INT, props));
-    props.fields.put("bigInt", new FieldInfo("int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT, props));
-    props.fields.put("f", new FieldInfo("float", "f", 32, floatVals, TypeProtos.MinorType.FLOAT4, props));
-    props.fields.put("d", new FieldInfo("double", "d", 64, doubleVals, TypeProtos.MinorType.FLOAT8, props));
-    props.fields.put("b", new FieldInfo("boolean", "b", 1, boolVals, TypeProtos.MinorType.BIT, props));
-    props.fields.put("bin", new FieldInfo("binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY, props));
-    props.fields.put("bin2", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
-  }
-
-  private void populatePigTPCHCustomerFields(ParquetTestProperties props){
-    // all of the data in the fieldInfo constructors doesn't matter because the file is generated outside the test
-    props.fields.put("C_CUSTKEY", new FieldInfo("int32", "integer", 32, intVals, TypeProtos.MinorType.INT, props));
-    props.fields.put("C_NATIONKEY", new FieldInfo("int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT, props));
-    props.fields.put("C_ACCTBAL", new FieldInfo("float", "f", 32, floatVals, TypeProtos.MinorType.FLOAT4, props));
-    props.fields.put("C_NAME", new FieldInfo("double", "d", 64, doubleVals, TypeProtos.MinorType.FLOAT8, props));
-    props.fields.put("C_ADDRESS", new FieldInfo("boolean", "b", 1, boolVals, TypeProtos.MinorType.BIT, props));
-    props.fields.put("C_PHONE", new FieldInfo("binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY, props));
-    props.fields.put("C_MKTSEGMENT", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
-    props.fields.put("C_COMMENT", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
-  }
-
-  private void populatePigTPCHSupplierFields(ParquetTestProperties props){
-    // all of the data in the fieldInfo constructors doesn't matter because the file is generated outside the test
-    props.fields.put("S_SUPPKEY", new FieldInfo("int32", "integer", 32, intVals, TypeProtos.MinorType.INT, props));
-    props.fields.put("S_NATIONKEY", new FieldInfo("int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT, props));
-    props.fields.put("S_ACCTBAL", new FieldInfo("float", "f", 32, floatVals, TypeProtos.MinorType.FLOAT4, props));
-    props.fields.put("S_NAME", new FieldInfo("double", "d", 64, doubleVals, TypeProtos.MinorType.FLOAT8, props));
-    props.fields.put("S_ADDRESS", new FieldInfo("boolean", "b", 1, boolVals, TypeProtos.MinorType.BIT, props));
-    props.fields.put("S_PHONE", new FieldInfo("binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY, props));
-    props.fields.put("S_COMMENT", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
-  }
-
-  @Test
-  public void testMultipleRowGroups() throws Exception {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    ParquetTestProperties props = new ParquetTestProperties(3, 3000, DEFAULT_BYTES_PER_PAGE, fields);
-    populateFieldInfoMap(props);
-    testParquetFullEngine(true, "/parquet_scan_screen.json", "/tmp/test.parquet", 1, props);
-  }
-
-  // TODO - Test currently marked ignore to prevent breaking of the build process, requires a binary file that was
-  // generated using pig. Will need to find a good place to keep files like this.
-  // For now I will upload it to the JIRA as an attachment.
-  @Ignore
-  @Test
-  public void testNullableColumns() throws Exception {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    ParquetTestProperties props = new ParquetTestProperties(1, 3000000, DEFAULT_BYTES_PER_PAGE, fields);
-    Object[] boolVals = {true, null, null};
-    props.fields.put("a", new FieldInfo("boolean", "a", 1, boolVals, TypeProtos.MinorType.BIT, props));
-    testParquetFullEngine(false, "/parquet_nullable.json", "/tmp/nullable.parquet", 1, props);
-  }
-
-  @Ignore
-  @Test
-  public void testNullableColumnsVarLen() throws Exception {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    ParquetTestProperties props = new ParquetTestProperties(1, 3000000, DEFAULT_BYTES_PER_PAGE, fields);
-    byte[] val = {'b'};
-//    Object[] boolVals = { val, null, null};
-//    Object[] boolVals = { null, null, null};
-    Object[] boolVals = { val, val, val};
-    props.fields.put("a", new FieldInfo("boolean", "a", 1, boolVals, TypeProtos.MinorType.BIT, props));
-    testParquetFullEngine(false, "/parquet_nullable_varlen.json", "/tmp/nullable.parquet", 1, props);
-  }
-
-  /**
-   * Tests the attribute in a scan node to limit the columns read by a scan.
-   *
-   * The functionality of selecting all columns is tested in all of the other tests that leave out the attribute.
-   * @throws Exception
-   */
-  @Test
-  public void testSelectColumnRead() throws Exception {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    ParquetTestProperties props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
-    // generate metatdata for a series of test columns, these columns are all generated in the test file
-    populateFieldInfoMap(props);
-    generateParquetFile("/tmp/test.parquet", props);
-    fields.clear();
-    // create a new object to describe the dataset expected out of the scan operation
-    // the fields added below match those requested in the plan specified in parquet_selective_column_read.json
-    // that is used below in the test query
-    props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
-    props.fields.put("integer", new FieldInfo("int32", "integer", 32, intVals, TypeProtos.MinorType.INT, props));
-    props.fields.put("bigInt", new FieldInfo("int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT, props));
-    props.fields.put("bin", new FieldInfo("binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY, props));
-    props.fields.put("bin2", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
-    testParquetFullEngineEventBased(false, "/parquet_selective_column_read.json", null, "/tmp/test.parquet", 1, props, false);
-  }
-
-
-  @Test
-  public void testMultipleRowGroupsAndReads() throws Exception {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    ParquetTestProperties props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
-    populateFieldInfoMap(props);
-    String readEntries = "";
-    // number of times to read the file
-    int i = 3;
-    for (int j = 0; j < i; j++){
-      readEntries += "{path: \"/tmp/test.parquet\"}";
-      if (j < i - 1)
-        readEntries += ",";
-    }
-    testParquetFullEngineEventBased(true, "/parquet_scan_screen_read_entry_replace.json", readEntries,
-        "/tmp/test.parquet", i, props, true);
-  }
-
-  // requires binary file generated by pig from TPCH data, also have to disable assert where data is coming in
-  @Ignore
-  @Test
-  public void testMultipleRowGroupsAndReadsPigError() throws Exception {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    ParquetTestProperties props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
-    populatePigTPCHCustomerFields(props);
-//    populatePigTPCHSupplierFields(props);
-    String readEntries = "";
-    // number of times to read the file
-    int i = 1;
-    for (int j = 0; j < i; j++){
-      readEntries += "{path: \"/tmp/tpc-h/customer\"}";
-      if (j < i - 1)
-        readEntries += ",";
-    }
-    testParquetFullEngineEventBased(false, "/parquet_scan_screen_read_entry_replace.json", readEntries,
-        "/tmp/test.parquet", i, props, true);
-  }
-/*
-  @Test
-  public void testMultipleRowGroupsEvent() throws Exception {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    ParquetTestProperties props = new ParquetTestProperties(10, 30000, DEFAULT_BYTES_PER_PAGE, fields);
-    props.fields.put("a", new FieldInfo("a", "asdf", 1, new Object[3], TypeProtos.MinorType.BIGINT, props));
-    testParquetFullEngineEventBased(false, "/parquet_scan_screen.json", "/tmp/out", 1, props);
-  }
-
-*/
-  private class ParquetTestProperties{
-    int numberRowGroups;
-    int recordsPerRowGroup;
-    int bytesPerPage = 1024 * 1024 * 1;
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-
-    public ParquetTestProperties(int numberRowGroups, int recordsPerRowGroup, int bytesPerPage,
-                                 HashMap<String, FieldInfo> fields){
-      this.numberRowGroups = numberRowGroups;
-      this.recordsPerRowGroup = recordsPerRowGroup;
-      this.bytesPerPage = bytesPerPage;
-      this.fields = fields;
-    }
-
-  }
-
-  private static class FieldInfo {
-
-    String parquetType;
-    String name;
-    int bitLength;
-    int numberOfPages;
-    Object[] values;
-    TypeProtos.MinorType type;
-
-    FieldInfo(String parquetType, String name, int bitLength, Object[] values, TypeProtos.MinorType type, ParquetTestProperties props){
-      this.parquetType = parquetType;
-      this.name = name;
-      this.bitLength  = bitLength;
-      this.numberOfPages = Math.max(1, (int) Math.ceil( ((long) props.recordsPerRowGroup) * bitLength / 8.0 / props.bytesPerPage));
-      this.values = values;
-      // generator is designed to use 3 values
-      assert values.length == 3;
-      this.type = type;
-    }
-  }
-
-  private String getResource(String resourceName) {
-    return "resource:" + resourceName;
-  }
-
-  public void generateParquetFile(String filename, ParquetTestProperties props) throws Exception {
-
-    int currentBooleanByte = 0;
-    WrapAroundCounter booleanBitCounter = new WrapAroundCounter(7);
-
-    Configuration configuration = new Configuration();
-    configuration.set(JsonSchemaProvider.HADOOP_DEFAULT_NAME, "file:///");
-    //"message m { required int32 integer; required int64 integer64; required boolean b; required float f; required double d;}"
-
-    FileSystem fs = FileSystem.get(configuration);
-    Path path = new Path(filename);
-    if (fs.exists(path)) fs.delete(path, false);
-
-
-    String messageSchema = "message m {";
-    for (FieldInfo fieldInfo : props.fields.values()) {
-      messageSchema += " required " + fieldInfo.parquetType + " " + fieldInfo.name + ";";
-    }
-    // remove the last semicolon, java really needs a join method for strings...
-    // TODO - nvm apparently it requires a semicolon after every field decl, might want to file a bug
-    //messageSchema = messageSchema.substring(schemaType, messageSchema.length() - 1);
-    messageSchema += "}";
-
-    MessageType schema = MessageTypeParser.parseMessageType(messageSchema);
-
-    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;
-    ParquetFileWriter w = new ParquetFileWriter(configuration, schema, path);
-    w.start();
-    HashMap<String, Integer> columnValuesWritten = new HashMap();
-    int valsWritten;
-    for (int k = 0; k < props.numberRowGroups; k++){
-      w.startBlock(1);
-      currentBooleanByte = 0;
-      booleanBitCounter.reset();
-
-      for (FieldInfo fieldInfo : props.fields.values()) {
-
-        if ( ! columnValuesWritten.containsKey(fieldInfo.name)){
-          columnValuesWritten.put((String) fieldInfo.name, 0);
-          valsWritten = 0;
-        } else {
-          valsWritten = columnValuesWritten.get(fieldInfo.name);
-        }
-
-        String[] path1 = {(String) fieldInfo.name};
-        ColumnDescriptor c1 = schema.getColumnDescription(path1);
-
-        w.startColumn(c1, props.recordsPerRowGroup, codec);
-        int valsPerPage = (int) Math.ceil(props.recordsPerRowGroup / (float) fieldInfo.numberOfPages);
-        byte[] bytes;
-        // for variable length binary fields
-        int bytesNeededToEncodeLength = 4;
-        if ((int) fieldInfo.bitLength > 0) {
-          bytes = new byte[(int) Math.ceil(valsPerPage * (int) fieldInfo.bitLength / 8.0)];
-        } else {
-          // the twelve at the end is to account for storing a 4 byte length with each value
-          int totalValLength = ((byte[]) fieldInfo.values[0]).length + ((byte[]) fieldInfo.values[1]).length + ((byte[]) fieldInfo.values[2]).length + 3 * bytesNeededToEncodeLength;
-          // used for the case where there is a number of values in this row group that is not divisible by 3
-          int leftOverBytes = 0;
-          if ( valsPerPage % 3 > 0 ) leftOverBytes += ((byte[])fieldInfo.values[1]).length + bytesNeededToEncodeLength;
-          if ( valsPerPage % 3 > 1 ) leftOverBytes += ((byte[])fieldInfo.values[2]).length + bytesNeededToEncodeLength;
-          bytes = new byte[valsPerPage / 3 * totalValLength + leftOverBytes];
-        }
-        int bytesPerPage = (int) (valsPerPage * ((int) fieldInfo.bitLength / 8.0));
-        int bytesWritten = 0;
-        for (int z = 0; z < (int) fieldInfo.numberOfPages; z++, bytesWritten = 0) {
-          for (int i = 0; i < valsPerPage; i++) {
-            //System.out.print(i + ", " + (i % 25 == 0 ? "\n gen " + fieldInfo.name + ": " : ""));
-            if (fieldInfo.values[0] instanceof Boolean) {
-
-              bytes[currentBooleanByte] |= bitFields[booleanBitCounter.val] & ((boolean) fieldInfo.values[valsWritten % 3]
-                  ? allBitsTrue : allBitsFalse);
-              booleanBitCounter.increment();
-              if (booleanBitCounter.val == 0) {
-                currentBooleanByte++;
-              }
-              valsWritten++;
-              if (currentBooleanByte > bytesPerPage) break;
-            } else {
-              if (fieldInfo.values[valsWritten % 3] instanceof byte[]){
-                System.arraycopy(ByteArrayUtil.toByta(((byte[])fieldInfo.values[valsWritten % 3]).length),
-                    0, bytes, bytesWritten, bytesNeededToEncodeLength);
-                try{
-                System.arraycopy(fieldInfo.values[valsWritten % 3],
-                    0, bytes, bytesWritten + bytesNeededToEncodeLength, ((byte[])fieldInfo.values[valsWritten % 3]).length);
-                }
-                catch (Exception ex){
-                  Math.min(4, 5);
-                }
-                bytesWritten += ((byte[])fieldInfo.values[valsWritten % 3]).length + bytesNeededToEncodeLength;
-              }
-              else{
-                System.arraycopy( ByteArrayUtil.toByta(fieldInfo.values[valsWritten % 3]),
-                    0, bytes, i * ((int) fieldInfo.bitLength / 8), (int) fieldInfo.bitLength / 8);
-              }
-              valsWritten++;
-            }
-
-          }
-          w.writeDataPage((int) (props.recordsPerRowGroup / (int) fieldInfo.numberOfPages), bytes.length, BytesInput.from(bytes), PLAIN, PLAIN, PLAIN);
-          currentBooleanByte = 0;
-        }
-        w.endColumn();
-        columnValuesWritten.remove((String) fieldInfo.name);
-        columnValuesWritten.put((String) fieldInfo.name, valsWritten);
-      }
-
-      w.endBlock();
-    }
-    w.end(new HashMap<String, String>());
-    logger.debug("Finished generating parquet file.");
-  }
-
-  private class ParquetResultListener implements UserResultsListener {
-    private SettableFuture<Void> future = SettableFuture.create();
-    int count = 0;
-    RecordBatchLoader batchLoader;
-
-    int batchCounter = 1;
-    HashMap<String, Integer> valuesChecked = new HashMap();
-    ParquetTestProperties props;
-
-    ParquetResultListener(RecordBatchLoader batchLoader, ParquetTestProperties props){
-      this.batchLoader = batchLoader;
-      this.props = props;
-    }
-
-    @Override
-    public void submissionFailed(RpcException ex) {
-      logger.debug("Submission failed.", ex);
-      future.setException(ex);
-    }
-
-    @Override
-    public void resultArrived(QueryResultBatch result) {
-      logger.debug("result arrived in test batch listener.");
-      if(result.getHeader().getIsLastChunk()){
-        future.set(null);
-      }
-      int columnValCounter = 0;
-      int i = 0;
-      FieldInfo currentField;
-      count += result.getHeader().getRowCount();
-      boolean schemaChanged = false;
-      try {
-        schemaChanged = batchLoader.load(result.getHeader().getDef(), result.getData());
-      } catch (SchemaChangeException e) {
-        throw new RuntimeException(e);
-      }
-
-      int recordCount = 0;
-      // print headers.
-      if (schemaChanged) {
-      } // do not believe any change is needed for when the schema changes, with the current mock scan use case
-
-      for (VectorWrapper vw : batchLoader) {
-        ValueVector vv = vw.getValueVector();
-        currentField = props.fields.get(vv.getField().getName());
-        if (VERBOSE_DEBUG){
-          System.out.println("\n" + (String) currentField.name);
-        }
-        if ( ! valuesChecked.containsKey(vv.getField().getName())){
-          valuesChecked.put(vv.getField().getName(), 0);
-          columnValCounter = 0;
-        } else {
-          columnValCounter = valuesChecked.get(vv.getField().getName());
-        }
-        for (int j = 0; j < ((BaseDataValueVector)vv).getAccessor().getValueCount(); j++) {
-          if (VERBOSE_DEBUG){
-            System.out.print(vv.getAccessor().getObject(j) + ", " + (j % 25 == 0 ? "\n batch:" + batchCounter + " v:" + j + " - " : ""));
-          }
-          assertField(vv, j, (TypeProtos.MinorType) currentField.type,
-              currentField.values[columnValCounter % 3], (String) currentField.name + "/");
-          columnValCounter++;
-        }
-        if (VERBOSE_DEBUG){
-          System.out.println("\n" + ((BaseDataValueVector)vv).getAccessor().getValueCount());
-        }
-        valuesChecked.remove(vv.getField().getName());
-        valuesChecked.put(vv.getField().getName(), columnValCounter);
-      }
-
-      if (VERBOSE_DEBUG){
-        for (i = 0; i < batchLoader.getRecordCount(); i++) {
-          recordCount++;
-          if (i % 50 == 0){
-            System.out.println();
-            for (VectorWrapper vw : batchLoader) {
-              ValueVector v = vw.getValueVector();
-              System.out.print(pad(v.getField().getName(), 20) + " ");
-
-            }
-            System.out.println();
-            System.out.println();
-          }
-
-          for (VectorWrapper vw : batchLoader) {
-            ValueVector v = vw.getValueVector();
-            System.out.print(pad(v.getAccessor().getObject(i).toString(), 20) + " ");
-          }
-          System.out.println(
-
-          );
-        }
-      }
-      batchCounter++;
-      if(result.getHeader().getIsLastChunk()){
-        future.set(null);
-      }
-    }
-
-    public void getResults() throws RpcException{
-      try{
-        future.get();
-      }catch(Throwable t){
-        throw RpcException.mapException(t);
-      }
-    }
-
-    @Override
-    public void queryIdArrived(QueryId queryId) {
-    }
-  }
-
-
-
-  // specific tests should call this method, but it is not marked as a test itself intentionally
-  public void testParquetFullEngineEventBased(boolean generateNew, String plan, String filename, int numberOfTimesRead /* specified in json plan */, ParquetTestProperties props) throws Exception{
-    testParquetFullEngineEventBased(generateNew, plan, null, filename, numberOfTimesRead, props, true);
-  }
-
-  // specific tests should call this method, but it is not marked as a test itself intentionally
-  public void testParquetFullEngineEventBased(boolean generateNew, String plan, String readEntries, String filename,
-                                              int numberOfTimesRead /* specified in json plan */, ParquetTestProperties props,
-                                              boolean runAsLogicalPlan) throws Exception{
-    RemoteServiceSet serviceSet = RemoteServiceSet.getLocalServiceSet();
-
-    if (generateNew) generateParquetFile(filename, props);
-
-    DrillConfig config = DrillConfig.create();
-
-    try(Drillbit bit1 = new Drillbit(config, serviceSet); DrillClient client = new DrillClient(config, serviceSet.getCoordinator());){
-      bit1.run();
-      client.connect();
-      RecordBatchLoader batchLoader = new RecordBatchLoader(bit1.getContext().getAllocator());
-      ParquetResultListener resultListener = new ParquetResultListener(batchLoader, props);
-      long C = System.nanoTime();
-      String planText = Files.toString(FileUtils.getResourceAsFile(plan), Charsets.UTF_8);
-      // substitute in the string for the read entries, allows reuse of the plan file for several tests
-      if (readEntries != null) {
-        planText = planText.replaceFirst( "&REPLACED_IN_PARQUET_TEST&", readEntries);
-      }
-      if (runAsLogicalPlan)
-        client.runQuery(UserProtos.QueryType.LOGICAL, planText, resultListener);
-      else
-        client.runQuery(UserProtos.QueryType.PHYSICAL, planText, resultListener);
-      resultListener.getResults();
-      for (String s : resultListener.valuesChecked.keySet()) {
-        assertEquals("Record count incorrect for column: " + s,
-            props.recordsPerRowGroup * props.numberRowGroups * numberOfTimesRead, (long) resultListener.valuesChecked.get(s));
-        logger.debug("Column {}, Values read:{}", s, resultListener.valuesChecked.get(s));
-      }
-      long D = System.nanoTime();
-      System.out.println(String.format("Took %f s to run query", (float)(D-C) / 1E9));
-    }
-  }
-
-  // specific tests should call this method, but it is not marked as a test itself intentionally
-  public void testParquetFullEngine(boolean generateNew, String plan, String filename, int numberOfTimesRead /* specified in json plan */, ParquetTestProperties props) throws Exception{
-    testParquetFullEngine(generateNew, plan, null, filename, numberOfTimesRead, props);
-  }
-
-  // specific tests should call this method, but it is not marked as a test itself intentionally
-  public void testParquetFullEngine(boolean generateNew, String plan, String readEntries, String filename,
-                                    int numberOfTimesRead /* specified in json plan */, ParquetTestProperties props) throws Exception{
-    RemoteServiceSet serviceSet = RemoteServiceSet.getLocalServiceSet();
-
-    if (generateNew) generateParquetFile(filename, props);
-
-    DrillConfig config = DrillConfig.create();
-
-    try(Drillbit bit1 = new Drillbit(config, serviceSet); DrillClient client = new DrillClient(config, serviceSet.getCoordinator())) {
-      long A = System.nanoTime();
-      bit1.run();
-      long B = System.nanoTime();
-      client.connect();
-      long C = System.nanoTime();
-      List<QueryResultBatch> results;
-      // insert a variable number of reads
-      if (readEntries != null){
-        results = client.runQuery(UserProtos.QueryType.LOGICAL, (Files.toString(FileUtils.getResourceAsFile(plan), Charsets.UTF_8).replaceFirst( "&REPLACED_IN_PARQUET_TEST&", readEntries)));
-      }
-      else{
-        results = client.runQuery(UserProtos.QueryType.LOGICAL, Files.toString(FileUtils.getResourceAsFile(plan), Charsets.UTF_8));
-      }
-//      List<QueryResultBatch> results = client.runQuery(UserProtos.QueryType.PHYSICAL, Files.toString(FileUtils.getResourceAsFile("/parquet_scan_union_screen_physical.json"), Charsets.UTF_8));
-      long D = System.nanoTime();
-      System.out.println(String.format("Took %f s to start drillbit", (float)(B-A) / 1E9));
-      System.out.println(String.format("Took %f s to connect", (float)(C-B) / 1E9));
-      System.out.println(String.format("Took %f s to run query", (float)(D-C) / 1E9));
-      //List<QueryResultBatch> results = client.runQuery(UserProtos.QueryType.PHYSICAL, Files.toString(FileUtils.getResourceAsFile("/parquet_scan_union_screen_physical.json"), Charsets.UTF_8));
-      int count = 0;
-//      RecordBatchLoader batchLoader = new RecordBatchLoader(new BootStrapContext(config).getAllocator());
-      RecordBatchLoader batchLoader = new RecordBatchLoader(bit1.getContext().getAllocator());
-      byte[] bytes;
-
-      int batchCounter = 1;
-      int columnValCounter = 0;
-      int i = 0;
-      FieldInfo currentField;
-      HashMap<String, Integer> valuesChecked = new HashMap();
-      for(QueryResultBatch b : results){
-
-        count += b.getHeader().getRowCount();
-        boolean schemaChanged = batchLoader.load(b.getHeader().getDef(), b.getData());
-
-        int recordCount = 0;
-        // print headers.
-        if (schemaChanged) {
-        } // do not believe any change is needed for when the schema changes, with the current mock scan use case
-
-        for (VectorWrapper vw : batchLoader) {
-          ValueVector vv = vw.getValueVector();
-          currentField = props.fields.get(vv.getField().getName());
-          if (VERBOSE_DEBUG){
-            System.out.println("\n" + (String) currentField.name);
-          }
-          if ( ! valuesChecked.containsKey(vv.getField().getName())){
-            valuesChecked.put(vv.getField().getName(), 0);
-            columnValCounter = 0;
-          } else {
-            columnValCounter = valuesChecked.get(vv.getField().getName());
-          }
-          for (int j = 0; j < vv.getAccessor().getValueCount(); j++) {
-            if (VERBOSE_DEBUG){
-              System.out.print(vv.getAccessor().getObject(j) + ", " + (j % 25 == 0 ? "\n batch:" + batchCounter + " v:" + j + " - " : ""));
-            }
-            assertField(vv, j, currentField.type,
-                currentField.values[columnValCounter % 3], currentField.name + "/");
-            columnValCounter++;
-          }
-          if (VERBOSE_DEBUG){
-            System.out.println("\n" + vv.getAccessor().getValueCount());
-          }
-          valuesChecked.remove(vv.getField().getName());
-          valuesChecked.put(vv.getField().getName(), columnValCounter);
-        }
-
-        if (VERBOSE_DEBUG){
-          for (i = 0; i < batchLoader.getRecordCount(); i++) {
-            recordCount++;
-            if (i % 50 == 0){
-              System.out.println();
-              for (VectorWrapper vw : batchLoader) {
-                ValueVector v = vw.getValueVector();
-                System.out.print(pad(v.getField().getName(), 20) + " ");
-
-              }
-              System.out.println();
-              System.out.println();
-            }
-
-            for (VectorWrapper vw : batchLoader) {
-              ValueVector v = vw.getValueVector();
-              System.out.print(pad(v.getAccessor().getObject(i) + "", 20) + " ");
-            }
-            System.out.println(
-
-            );
-          }
-        }
-        batchCounter++;
-      }
-      for (String s : valuesChecked.keySet()) {
-        assertEquals("Record count incorrect for column: " + s, props.recordsPerRowGroup * props.numberRowGroups * numberOfTimesRead, (long) valuesChecked.get(s));
-      }
-      assert valuesChecked.keySet().size() > 0;
-    }
-  }
-
-  public String pad(String value, int length) {
-    return pad(value, length, " ");
-  }
-
-  public String pad(String value, int length, String with) {
-    StringBuilder result = new StringBuilder(length);
-    result.append(value);
-
-    while (result.length() < length) {
-      result.insert(0, with);
-    }
-
-    return result.toString();
-  }
-
-  class MockOutputMutator implements OutputMutator {
-    List<MaterializedField> removedFields = Lists.newArrayList();
-    List<ValueVector> addFields = Lists.newArrayList();
-
-    @Override
-    public void removeField(MaterializedField field) throws SchemaChangeException {
-      removedFields.add(field);
-    }
-
-    @Override
-    public void addField(ValueVector vector) throws SchemaChangeException {
-      addFields.add(vector);
-    }
-
-    @Override
-    public void removeAllFields() {
-      addFields.clear();
-    }
-
-    @Override
-    public void setNewSchema() throws SchemaChangeException {
-    }
-
-    List<MaterializedField> getRemovedFields() {
-      return removedFields;
-    }
-
-    List<ValueVector> getAddFields() {
-      return addFields;
-    }
-  }
-
-  private <T> void assertField(ValueVector valueVector, int index, TypeProtos.MinorType expectedMinorType, Object value, String name) {
-    assertField(valueVector, index, expectedMinorType, value, name, 0);
-  }
-
-  private <T> void assertField(ValueVector valueVector, int index, TypeProtos.MinorType expectedMinorType, T value, String name, int parentFieldId) {
-//    UserBitShared.FieldMetadata metadata = valueVector.getMetadata();
-//    SchemaDefProtos.FieldDef def = metadata.getDef();
-//    assertEquals(expectedMinorType, def.getMajorType().getMinorType());
-//    assertEquals(name, def.getNameList().get(0).getName());
-//    assertEquals(parentFieldId, def.getParentId());
-
-    if (expectedMinorType == TypeProtos.MinorType.MAP) {
-      return;
-    }
-
-    T val = (T) valueVector.getAccessor().getObject(index);
-    if (val instanceof String){
-      assertEquals(value, val);
-    }
-    else if (val instanceof byte[]) {
-      assertTrue(Arrays.equals((byte[]) value, (byte[]) val));
-    } else {
-      assertEquals(value, val);
-    }
-  }
-
-  private class WrapAroundCounter {
-
-    int maxVal;
-    int val;
-
-    public WrapAroundCounter(int maxVal) {
-      this.maxVal = maxVal;
-    }
-
-    public int increment() {
-      val++;
-      if (val > maxVal) {
-        val = 0;
-      }
-      return val;
-    }
-
-    public void reset() {
-      val = 0;
-    }
-
-  }
-}
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/TestOutputMutator.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/TestOutputMutator.java
new file mode 100644
index 0000000..51a0b0b
--- /dev/null
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/TestOutputMutator.java
@@ -0,0 +1,74 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.drill.exec.store;
+
+import java.util.Iterator;
+import java.util.Map;
+
+import org.apache.drill.exec.exception.SchemaChangeException;
+import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;
+import org.apache.drill.exec.record.MaterializedField;
+import org.apache.drill.exec.record.VectorContainer;
+import org.apache.drill.exec.record.VectorWrapper;
+import org.apache.drill.exec.vector.ValueVector;
+
+import com.google.common.collect.Maps;
+
+public class TestOutputMutator implements OutputMutator, Iterable<VectorWrapper<?>> {
+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(TestOutputMutator.class);
+
+  private final VectorContainer container = new VectorContainer();
+  private final Map<MaterializedField, ValueVector> fieldVectorMap = Maps.newHashMap();
+
+  public void removeField(MaterializedField field) throws SchemaChangeException {
+    ValueVector vector = fieldVectorMap.remove(field);
+    if (vector == null)
+      throw new SchemaChangeException("Failure attempting to remove an unknown field.");
+    container.remove(vector);
+    vector.close();
+  }
+
+  public void addField(ValueVector vector) {
+    container.add(vector);
+    fieldVectorMap.put(vector.getField(), vector);
+  }
+
+  @Override
+  public void removeAllFields() {
+    for (VectorWrapper<?> vw : container) {
+      vw.clear();
+    }
+    container.clear();
+    fieldVectorMap.clear();
+  }
+
+  @Override
+  public void setNewSchema() throws SchemaChangeException {
+    container.buildSchema(SelectionVectorMode.NONE);
+  }
+
+  public Iterator<VectorWrapper<?>> iterator() {
+    return container.iterator();
+  }
+
+  public void clear(){
+    removeAllFields();
+  }
+
+}
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/FieldInfo.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/FieldInfo.java
new file mode 100644
index 0000000..34f60ba
--- /dev/null
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/FieldInfo.java
@@ -0,0 +1,40 @@
+/*******************************************************************************
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ******************************************************************************/
+package org.apache.drill.exec.store.parquet;
+
+import org.apache.drill.common.types.TypeProtos;
+
+public class FieldInfo {
+  String parquetType;
+  String name;
+  int bitLength;
+  int numberOfPages;
+  Object[] values;
+  TypeProtos.MinorType type;
+
+  FieldInfo(String parquetType, String name, int bitLength, Object[] values, TypeProtos.MinorType type, ParquetTestProperties props){
+    this.parquetType = parquetType;
+    this.name = name;
+    this.bitLength  = bitLength;
+    this.numberOfPages = Math.max(1, (int) Math.ceil( ((long) props.recordsPerRowGroup) * bitLength / 8.0 / props.bytesPerPage));
+    this.values = values;
+    // generator is designed to use 3 values
+    assert values.length == 3;
+    this.type = type;
+  }
+}
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java
index ab29a9f..d9bdabb 100644
--- a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java
@@ -17,6 +17,8 @@
  */
 package org.apache.drill.exec.store.parquet;
 
+import static org.apache.drill.exec.store.parquet.TestFileGenerator.bytesPerPage;
+import static org.apache.drill.exec.store.parquet.TestFileGenerator.populateFieldInfoMap;
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
@@ -29,12 +31,19 @@ import java.util.List;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
 
+import mockit.Injectable;
 import org.apache.drill.common.config.DrillConfig;
+import org.apache.drill.common.expression.ExpressionPosition;
+import org.apache.drill.common.expression.FieldReference;
+import org.apache.drill.common.expression.SchemaPath;
 import org.apache.drill.common.types.TypeProtos;
 import org.apache.drill.common.util.FileUtils;
 import org.apache.drill.exec.client.DrillClient;
 import org.apache.drill.exec.exception.SchemaChangeException;
+import org.apache.drill.exec.expr.fn.FunctionImplementationRegistry;
+import org.apache.drill.exec.ops.FragmentContext;
 import org.apache.drill.exec.physical.impl.OutputMutator;
+import org.apache.drill.exec.proto.ExecProtos;
 import org.apache.drill.exec.proto.UserBitShared.QueryId;
 import org.apache.drill.exec.proto.UserProtos;
 import org.apache.drill.exec.record.MaterializedField;
@@ -43,11 +52,16 @@ import org.apache.drill.exec.record.VectorWrapper;
 import org.apache.drill.exec.rpc.RpcException;
 import org.apache.drill.exec.rpc.user.QueryResultBatch;
 import org.apache.drill.exec.rpc.user.UserResultsListener;
+import org.apache.drill.exec.rpc.user.UserServer;
 import org.apache.drill.exec.server.Drillbit;
+import org.apache.drill.exec.server.DrillbitContext;
 import org.apache.drill.exec.server.RemoteServiceSet;
-import org.apache.drill.exec.store.parquet.TestFileGenerator.FieldInfo;
-import org.apache.drill.exec.vector.BaseDataValueVector;
+import org.apache.drill.exec.store.CachedSingleFileSystem;
+import org.apache.drill.exec.store.TestOutputMutator;
 import org.apache.drill.exec.vector.ValueVector;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 import org.junit.BeforeClass;
 import org.junit.Ignore;
 import org.junit.Test;
@@ -56,7 +70,9 @@ import parquet.bytes.BytesInput;
 import parquet.column.page.Page;
 import parquet.column.page.PageReadStore;
 import parquet.column.page.PageReader;
+import parquet.hadoop.CodecFactoryExposer;
 import parquet.hadoop.Footer;
+import parquet.hadoop.ParquetFileReader;
 import parquet.hadoop.metadata.ParquetMetadata;
 import parquet.schema.MessageType;
 
@@ -64,30 +80,33 @@ import com.google.common.base.Charsets;
 import com.google.common.base.Stopwatch;
 import com.google.common.collect.Lists;
 import com.google.common.io.Files;
-import com.google.common.util.concurrent.SettableFuture;
 
 public class ParquetRecordReaderTest {
   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ParquetRecordReaderTest.class);
 
-  private boolean VERBOSE_DEBUG = false;
+  static boolean VERBOSE_DEBUG = false;
   private boolean checkValues = true;
 
   static final int numberRowGroups = 1;
   static final int recordsPerRowGroup = 300;
+  static int DEFAULT_BYTES_PER_PAGE = 1024 * 1024 * 1;
   static final String fileName = "/tmp/parquet_test_file_many_types";
 
   @BeforeClass
   public static void generateFile() throws Exception{
     File f = new File(fileName);
-    if(!f.exists()) TestFileGenerator.generateParquetFile(fileName, numberRowGroups, recordsPerRowGroup);
+    ParquetTestProperties props = new ParquetTestProperties(numberRowGroups, recordsPerRowGroup, DEFAULT_BYTES_PER_PAGE, new HashMap<String, FieldInfo>());
+    populateFieldInfoMap(props);
+    if(!f.exists()) TestFileGenerator.generateParquetFile(fileName, props);
   }
- 
+
+
   @Test
-  public void testMultipleRowGroupsAndReads() throws Exception {
+  public void testMultipleRowGroupsAndReads3() throws Exception {
     String planName = "/parquet/parquet_scan_screen.json";
     testParquetFullEngineLocalPath(planName, fileName, 2, numberRowGroups, recordsPerRowGroup);
   }
-  
+
   @Test
   public void testMultipleRowGroupsAndReads2() throws Exception {
     String readEntries;
@@ -116,158 +135,13 @@ public class ParquetRecordReaderTest {
     testParquetFullEngineRemote(planName, fileName, 1, numberRowGroups, recordsPerRowGroup);
   }
 
-
-  private class ParquetResultListener implements UserResultsListener {
-    private SettableFuture<Void> future = SettableFuture.create();
-    RecordBatchLoader batchLoader;
-
-    int batchCounter = 1;
-    private final HashMap<String, Long> valuesChecked = new HashMap<>();
-    private final Map<String, FieldInfo> fields;
-    private final long totalRecords;
-    
-    ParquetResultListener(int recordsPerRowGroup, RecordBatchLoader batchLoader, int numberRowGroups, int numberOfTimesRead){
-      this.batchLoader = batchLoader;
-      this.fields = TestFileGenerator.getFieldMap(recordsPerRowGroup);
-      this.totalRecords = recordsPerRowGroup * numberRowGroups * numberOfTimesRead;
-    }
-
-    @Override
-    public void submissionFailed(RpcException ex) {
-      logger.debug("Submission failed.", ex);
-      future.setException(ex);
-    }
-
-    @Override
-    public void resultArrived(QueryResultBatch result) {
-      long columnValCounter = 0;
-      int i = 0;
-      FieldInfo currentField;
-
-      boolean schemaChanged = false;
-      try {
-        schemaChanged = batchLoader.load(result.getHeader().getDef(), result.getData());
-      } catch (SchemaChangeException e) {
-        logger.error("Failure while loading batch", e);
-      }
-
-      // print headers.
-      if (schemaChanged) {
-      } // do not believe any change is needed for when the schema changes, with the current mock scan use case
-
-      for (VectorWrapper<?> vw : batchLoader) {
-        ValueVector vv = vw.getValueVector();
-        currentField = fields.get(vv.getField().getName());
-        if (VERBOSE_DEBUG){
-          System.out.println("\n" + (String) currentField.name);
-        }
-        if ( ! valuesChecked.containsKey(vv.getField().getName())){
-          valuesChecked.put(vv.getField().getName(), (long) 0);
-          columnValCounter = 0;
-        } else {
-          columnValCounter = valuesChecked.get(vv.getField().getName());
-        }
-        for (int j = 0; j < ((BaseDataValueVector)vv).getAccessor().getValueCount(); j++) {
-          if (VERBOSE_DEBUG){
-            System.out.print(vv.getAccessor().getObject(j) + ", " + (j % 25 == 0 ? "\n batch:" + batchCounter + " v:" + j + " - " : ""));
-          }
-          if (checkValues) {
-            try {
-              assertField(vv, j, (TypeProtos.MinorType) currentField.type,
-                currentField.values[(int) (columnValCounter % 3)], (String) currentField.name + "/");
-            } catch (AssertionError e) { submissionFailed(new RpcException(e)); }
-          }
-          columnValCounter++;
-        }
-        if (VERBOSE_DEBUG){
-          System.out.println("\n" + ((BaseDataValueVector)vv).getAccessor().getValueCount());
-        }
-        valuesChecked.remove(vv.getField().getName());
-        valuesChecked.put(vv.getField().getName(), columnValCounter);
-      }
-      
-      
-      if (VERBOSE_DEBUG){
-        for (i = 0; i < batchLoader.getRecordCount(); i++) {
-          if (i % 50 == 0){
-            System.out.println();
-            for (VectorWrapper<?> vw : batchLoader) {
-              ValueVector v = vw.getValueVector();
-              System.out.print(pad(v.getField().getName(), 20) + " ");
-            }
-            System.out.println();
-            System.out.println();
-          }
-
-          for (VectorWrapper<?> vw : batchLoader) {
-            ValueVector v = vw.getValueVector();
-            System.out.print(pad(v.getAccessor().getObject(i).toString(), 20) + " ");
-          }
-          System.out.println(
-
-          );
-        }
-      }
-
-      for(VectorWrapper<?> vw : batchLoader){
-        vw.clear();
-      }
-      result.release();
-      
-      batchCounter++;
-      if(result.getHeader().getIsLastChunk()){
-        for (String s : valuesChecked.keySet()) {
-          try {
-          assertEquals("Record count incorrect for column: " + s, totalRecords, (long) valuesChecked.get(s));
-          } catch (AssertionError e) { submissionFailed(new RpcException(e)); }
-        }
-        
-        assert valuesChecked.keySet().size() > 0;
-        future.set(null);
-      }
-    }
-
-    public void get() throws RpcException{
-      try{
-        future.get();
-        return;
-      }catch(Throwable t){
-        throw RpcException.mapException(t);
-      }
-    }
-
-    @Override
-    public void queryIdArrived(QueryId queryId) {
-    }
-  }
-
-  
-  
-  
-  public void testParquetFullEngineRemote(String plan, String filename, int numberOfTimesRead /* specified in json plan */, int numberOfRowGroups, int recordsPerRowGroup) throws Exception{
-    
-    DrillConfig config = DrillConfig.create();
-
-    checkValues = false;
-
-    try(DrillClient client = new DrillClient(config);){
-      client.connect();
-      RecordBatchLoader batchLoader = new RecordBatchLoader(client.getAllocator());
-      ParquetResultListener resultListener = new ParquetResultListener(recordsPerRowGroup, batchLoader, numberOfRowGroups, numberOfTimesRead);
-      client.runQuery(UserProtos.QueryType.PHYSICAL, Files.toString(FileUtils.getResourceAsFile(plan), Charsets.UTF_8), resultListener);
-      resultListener.get();
-    }
-    
-  }
-  
-  
   public void testParquetFullEngineLocalPath(String planFileName, String filename, int numberOfTimesRead /* specified in json plan */, int numberOfRowGroups, int recordsPerRowGroup) throws Exception{
     testParquetFullEngineLocalText(Files.toString(FileUtils.getResourceAsFile(planFileName), Charsets.UTF_8), filename, numberOfTimesRead, numberOfRowGroups, recordsPerRowGroup);
   }
-  
+
   //specific tests should call this method, but it is not marked as a test itself intentionally
   public void testParquetFullEngineLocalText(String planText, String filename, int numberOfTimesRead /* specified in json plan */, int numberOfRowGroups, int recordsPerRowGroup) throws Exception{
-    
+
     RemoteServiceSet serviceSet = RemoteServiceSet.getLocalServiceSet();
 
     DrillConfig config = DrillConfig.create();
@@ -276,14 +150,17 @@ public class ParquetRecordReaderTest {
       bit1.run();
       client.connect();
       RecordBatchLoader batchLoader = new RecordBatchLoader(client.getAllocator());
-      ParquetResultListener resultListener = new ParquetResultListener(recordsPerRowGroup, batchLoader, numberOfRowGroups, numberOfTimesRead);
+      HashMap<String, FieldInfo> fields = new HashMap<>();
+      ParquetTestProperties props = new ParquetTestProperties(numberRowGroups, recordsPerRowGroup, DEFAULT_BYTES_PER_PAGE, fields);
+      TestFileGenerator.populateFieldInfoMap(props);
+      ParquetResultListener resultListener = new ParquetResultListener(batchLoader, props, numberOfTimesRead, true);
       Stopwatch watch = new Stopwatch().start();
       client.runQuery(UserProtos.QueryType.LOGICAL, planText, resultListener);
-      resultListener.get();
+      resultListener.getResults();
       System.out.println(String.format("Took %d ms to run query", watch.elapsed(TimeUnit.MILLISECONDS)));
 
     }
-    
+
   }
 
 
@@ -300,10 +177,13 @@ public class ParquetRecordReaderTest {
       bit1.run();
       client.connect();
       RecordBatchLoader batchLoader = new RecordBatchLoader(client.getAllocator());
-      ParquetResultListener resultListener = new ParquetResultListener(recordsPerRowGroup, batchLoader, numberOfRowGroups, numberOfTimesRead);
+      HashMap<String, FieldInfo> fields = new HashMap<>();
+      ParquetTestProperties props = new ParquetTestProperties(numberRowGroups, recordsPerRowGroup, DEFAULT_BYTES_PER_PAGE, fields);
+      TestFileGenerator.populateFieldInfoMap(props);
+      ParquetResultListener resultListener = new ParquetResultListener(batchLoader, props, numberOfTimesRead, true);
       Stopwatch watch = new Stopwatch().start();
       client.runQuery(UserProtos.QueryType.PHYSICAL, Files.toString(FileUtils.getResourceAsFile(planName), Charsets.UTF_8), resultListener);
-      resultListener.get();
+      resultListener.getResults();
       System.out.println(String.format("Took %d ms to run query", watch.elapsed(TimeUnit.MILLISECONDS)));
 
     }
@@ -325,6 +205,25 @@ public class ParquetRecordReaderTest {
     return result.toString();
   }
 
+  public void testParquetFullEngineRemote(String plan, String filename, int numberOfTimesRead /* specified in json plan */, int numberOfRowGroups, int recordsPerRowGroup) throws Exception{
+
+    DrillConfig config = DrillConfig.create();
+
+    checkValues = false;
+
+    try(DrillClient client = new DrillClient(config);){
+      client.connect();
+      RecordBatchLoader batchLoader = new RecordBatchLoader(client.getAllocator());
+      HashMap<String, FieldInfo> fields = new HashMap<>();
+      ParquetTestProperties props = new ParquetTestProperties(numberRowGroups, recordsPerRowGroup, DEFAULT_BYTES_PER_PAGE, fields);
+      TestFileGenerator.populateFieldInfoMap(props);
+      ParquetResultListener resultListener = new ParquetResultListener(batchLoader, props, numberOfTimesRead, true);
+      client.runQuery(UserProtos.QueryType.PHYSICAL, Files.toString(FileUtils.getResourceAsFile(plan), Charsets.UTF_8), resultListener);
+      resultListener.getResults();
+    }
+
+  }
+
   class MockOutputMutator implements OutputMutator {
     List<MaterializedField> removedFields = Lists.newArrayList();
     List<ValueVector> addFields = Lists.newArrayList();
@@ -357,24 +256,6 @@ public class ParquetRecordReaderTest {
     }
   }
 
-  private <T> void assertField(ValueVector valueVector, int index, TypeProtos.MinorType expectedMinorType, Object value, String name) {
-    assertField(valueVector, index, expectedMinorType, value, name, 0);
-  }
-
-  @SuppressWarnings("unchecked")
-  private <T> void assertField(ValueVector valueVector, int index, TypeProtos.MinorType expectedMinorType, T value, String name, int parentFieldId) {
-
-    if (expectedMinorType == TypeProtos.MinorType.MAP) {
-      return;
-    }
-    
-    T val = (T) valueVector.getAccessor().getObject(index);
-    if (val instanceof byte[]) {
-      assertTrue(Arrays.equals((byte[]) value, (byte[]) val));
-    } else {
-      assertEquals(value, val);
-    }
-  }
 
   private void validateFooters(final List<Footer> metadata) {
     logger.debug(metadata.toString());
@@ -390,8 +271,8 @@ public class ParquetRecordReaderTest {
       assertEquals(footer.getFile().getName(), keyValueMetaData.get(footer.getFile().getName()));
     }
   }
-  
-  
+
+
   private void validateContains(MessageType schema, PageReadStore pages, String[] path, int values, BytesInput bytes)
       throws IOException {
     PageReader pageReader = pages.getPageReader(schema.getColumnDescription(path));
@@ -401,5 +282,208 @@ public class ParquetRecordReaderTest {
   }
 
 
-  
+  @Test
+  public void testMultipleRowGroups() throws Exception {
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(3, 3000, DEFAULT_BYTES_PER_PAGE, fields);
+    populateFieldInfoMap(props);
+    testParquetFullEngineEventBased(true, "/parquet_scan_screen.json", "/tmp/test.parquet", 1, props);
+  }
+
+  // TODO - Test currently marked ignore to prevent breaking of the build process, requires a binary file that was
+  // generated using pig. Will need to find a good place to keep files like this.
+  // For now I will upload it to the JIRA as an attachment.
+  @Ignore
+  @Test
+  public void testNullableColumns() throws Exception {
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(1, 3000000, DEFAULT_BYTES_PER_PAGE, fields);
+    Object[] boolVals = {true, null, null};
+    props.fields.put("a", new FieldInfo("boolean", "a", 1, boolVals, TypeProtos.MinorType.BIT, props));
+    testParquetFullEngineEventBased(false, "/parquet_nullable.json", "/tmp/nullable_test.parquet", 1, props);
+  }
+
+  @Ignore
+  @Test
+  public void testNullableColumnsVarLen() throws Exception {
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(1, 300000, DEFAULT_BYTES_PER_PAGE, fields);
+    byte[] val = {'b'};
+    byte[] val2 = {'b', '2'};
+    byte[] val3 = { 'l','o','n','g','e','r',' ','s','t','r','i','n','g'};
+    Object[] boolVals = { val, val2, val3};
+    props.fields.put("a", new FieldInfo("boolean", "a", 1, boolVals, TypeProtos.MinorType.BIT, props));
+    testParquetFullEngineEventBased(false, "/parquet_nullable_varlen.json", "/tmp/nullable_varlen.parquet", 1, props);
+  }
+
+  @Test
+  public void testMultipleRowGroupsAndReads() throws Exception {
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
+    populateFieldInfoMap(props);
+    String readEntries = "";
+    // number of times to read the file
+    int i = 3;
+    for (int j = 0; j < i; j++){
+      readEntries += "{path: \"/tmp/test.parquet\"}";
+      if (j < i - 1)
+        readEntries += ",";
+    }
+    testParquetFullEngineEventBased(true, "/parquet_scan_screen_read_entry_replace.json", readEntries,
+        "/tmp/test.parquet", i, props);
+  }
+
+  // requires binary file generated by pig from TPCH data, also have to disable assert where data is coming in
+  @Ignore
+  @Test
+  public void testMultipleRowGroupsAndReadsPigError() throws Exception {
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(5, 300000, DEFAULT_BYTES_PER_PAGE, fields);
+    TestFileGenerator.populatePigTPCHCustomerFields(props);
+    String readEntries = "{path: \"/tmp/tpc-h/customer\"}";
+    testParquetFullEngineEventBased(false, false, "/parquet_scan_screen_read_entry_replace.json", readEntries,
+        "unused, no file is generated", 1, props, true);
+
+    fields = new HashMap();
+    props = new ParquetTestProperties(5, 300000, DEFAULT_BYTES_PER_PAGE, fields);
+    TestFileGenerator.populatePigTPCHSupplierFields(props);
+    readEntries = "{path: \"/tmp/tpc-h/supplier\"}";
+    testParquetFullEngineEventBased(false, false, "/parquet_scan_screen_read_entry_replace.json", readEntries,
+        "unused, no file is generated", 1, props, true);
+  }
+
+  @Test
+  public void testMultipleRowGroupsEvent() throws Exception {
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
+    populateFieldInfoMap(props);
+    testParquetFullEngineEventBased(true, "/parquet_scan_screen.json", "/tmp/test.parquet", 1, props);
+  }
+
+
+  /**
+   * Tests the attribute in a scan node to limit the columns read by a scan.
+   *
+   * The functionality of selecting all columns is tested in all of the other tests that leave out the attribute.
+   * @throws Exception
+   */
+  @Test
+  public void testSelectColumnRead() throws Exception {
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
+    // generate metatdata for a series of test columns, these columns are all generated in the test file
+    populateFieldInfoMap(props);
+    TestFileGenerator.generateParquetFile("/tmp/test.parquet", props);
+    fields.clear();
+    // create a new object to describe the dataset expected out of the scan operation
+    // the fields added below match those requested in the plan specified in parquet_selective_column_read.json
+    // that is used below in the test query
+    props = new ParquetTestProperties(4, 3000, DEFAULT_BYTES_PER_PAGE, fields);
+    props.fields.put("integer", new FieldInfo("int32", "integer", 32, TestFileGenerator.intVals, TypeProtos.MinorType.INT, props));
+    props.fields.put("bigInt", new FieldInfo("int64", "bigInt", 64, TestFileGenerator.longVals, TypeProtos.MinorType.BIGINT, props));
+    props.fields.put("bin", new FieldInfo("binary", "bin", -1, TestFileGenerator.binVals, TypeProtos.MinorType.VARBINARY, props));
+    props.fields.put("bin2", new FieldInfo("binary", "bin2", -1, TestFileGenerator.bin2Vals, TypeProtos.MinorType.VARBINARY, props));
+    testParquetFullEngineEventBased(true, false, "/parquet_selective_column_read.json", null, "/tmp/test.parquet", 1, props, false);
+  }
+
+  public static void main(String[] args) throws Exception{
+    // TODO - not sure why this has a main method, test below can be run directly
+    //new ParquetRecordReaderTest().testPerformance();
+  }
+
+  @Test
+  @Ignore
+  public void testPerformance(@Injectable final DrillbitContext bitContext,
+                              @Injectable UserServer.UserClientConnection connection) throws Exception {
+    DrillConfig c = DrillConfig.create();
+    FunctionImplementationRegistry registry = new FunctionImplementationRegistry(c);
+    FragmentContext context = new FragmentContext(bitContext, ExecProtos.FragmentHandle.getDefaultInstance(), connection, registry);
+
+//    new NonStrictExpectations() {
+//      {
+//        context.getAllocator(); result = BufferAllocator.getAllocator(DrillConfig.create());
+//      }
+//    };
+
+    final String fileName = "/tmp/parquet_test_performance.parquet";
+    HashMap<String, FieldInfo> fields = new HashMap<>();
+    ParquetTestProperties props = new ParquetTestProperties(1, 20 * 1000 * 1000, DEFAULT_BYTES_PER_PAGE, fields);
+    populateFieldInfoMap(props);
+    //generateParquetFile(fileName, props);
+
+    Configuration dfsConfig = new Configuration();
+    List<Footer> footers = ParquetFileReader.readFooters(dfsConfig, new Path(fileName));
+    Footer f = footers.iterator().next();
+
+    List<SchemaPath> columns = Lists.newArrayList();
+    columns.add(new SchemaPath("_MAP.integer", ExpressionPosition.UNKNOWN));
+    columns.add(new SchemaPath("_MAP.bigInt", ExpressionPosition.UNKNOWN));
+    columns.add(new SchemaPath("_MAP.f", ExpressionPosition.UNKNOWN));
+    columns.add(new SchemaPath("_MAP.d", ExpressionPosition.UNKNOWN));
+    columns.add(new SchemaPath("_MAP.b", ExpressionPosition.UNKNOWN));
+    columns.add(new SchemaPath("_MAP.bin", ExpressionPosition.UNKNOWN));
+    columns.add(new SchemaPath("_MAP.bin2", ExpressionPosition.UNKNOWN));
+    int totalRowCount = 0;
+
+    FileSystem fs = new CachedSingleFileSystem(fileName);
+    for(int i = 0; i < 25; i++){
+      ParquetRecordReader rr = new ParquetRecordReader(context, 256000, fileName, 0, fs,
+          new CodecFactoryExposer(dfsConfig), f.getParquetMetadata(), new FieldReference("_MAP",
+          ExpressionPosition.UNKNOWN), columns);
+      TestOutputMutator mutator = new TestOutputMutator();
+      rr.setup(mutator);
+      Stopwatch watch = new Stopwatch();
+      watch.start();
+
+      int rowCount = 0;
+      while ((rowCount = rr.next()) > 0) {
+        totalRowCount += rowCount;
+      }
+      System.out.println(String.format("Time completed: %s. ", watch.elapsed(TimeUnit.MILLISECONDS)));
+      rr.cleanup();
+    }
+    System.out.println(String.format("Total row count %s", totalRowCount));
+  }
+
+  // specific tests should call this method, but it is not marked as a test itself intentionally
+  public void testParquetFullEngineEventBased(boolean generateNew, String plan, String readEntries, String filename,
+                                              int numberOfTimesRead /* specified in json plan */, ParquetTestProperties props) throws Exception{
+    testParquetFullEngineEventBased(true, generateNew, plan, readEntries,filename,
+                                              numberOfTimesRead /* specified in json plan */, props, true);
+  }
+
+
+  // specific tests should call this method, but it is not marked as a test itself intentionally
+  public void testParquetFullEngineEventBased(boolean generateNew, String plan, String filename, int numberOfTimesRead /* specified in json plan */, ParquetTestProperties props) throws Exception{
+    testParquetFullEngineEventBased(true, generateNew, plan, null, filename, numberOfTimesRead, props, true);
+  }
+
+  // specific tests should call this method, but it is not marked as a test itself intentionally
+  public void testParquetFullEngineEventBased(boolean testValues, boolean generateNew, String plan, String readEntries, String filename,
+                                              int numberOfTimesRead /* specified in json plan */, ParquetTestProperties props,
+                                              boolean runAsLogicalPlan) throws Exception{
+    RemoteServiceSet serviceSet = RemoteServiceSet.getLocalServiceSet();
+    if (generateNew) TestFileGenerator.generateParquetFile(filename, props);
+    DrillConfig config = DrillConfig.create();
+    try(Drillbit bit1 = new Drillbit(config, serviceSet); DrillClient client = new DrillClient(config, serviceSet.getCoordinator());){
+      bit1.run();
+      client.connect();
+      RecordBatchLoader batchLoader = new RecordBatchLoader(bit1.getContext().getAllocator());
+      ParquetResultListener resultListener = new ParquetResultListener(batchLoader, props, numberOfTimesRead, testValues);
+      long C = System.nanoTime();
+      String planText = Files.toString(FileUtils.getResourceAsFile(plan), Charsets.UTF_8);
+      // substitute in the string for the read entries, allows reuse of the plan file for several tests
+      if (readEntries != null) {
+        planText = planText.replaceFirst( "&REPLACED_IN_PARQUET_TEST&", readEntries);
+      }
+      if (runAsLogicalPlan)
+        client.runQuery(UserProtos.QueryType.LOGICAL, planText, resultListener);
+      else
+        client.runQuery(UserProtos.QueryType.PHYSICAL, planText, resultListener);
+      resultListener.getResults();
+      long D = System.nanoTime();
+      System.out.println(String.format("Took %f s to run query", (float)(D-C) / 1E9));
+    }
+  }
+
 }
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetResultListener.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetResultListener.java
new file mode 100644
index 0000000..2e315d1
--- /dev/null
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetResultListener.java
@@ -0,0 +1,202 @@
+/*******************************************************************************
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ******************************************************************************/
+package org.apache.drill.exec.store.parquet;
+
+import com.google.common.base.Strings;
+import com.google.common.util.concurrent.SettableFuture;
+import org.apache.drill.common.types.TypeProtos;
+import org.apache.drill.exec.exception.SchemaChangeException;
+import org.apache.drill.exec.proto.UserBitShared;
+import org.apache.drill.exec.record.RecordBatchLoader;
+import org.apache.drill.exec.record.VectorWrapper;
+import org.apache.drill.exec.rpc.RpcException;
+import org.apache.drill.exec.rpc.user.QueryResultBatch;
+import org.apache.drill.exec.rpc.user.UserResultsListener;
+import org.apache.drill.exec.vector.BaseDataValueVector;
+import org.apache.drill.exec.vector.ValueVector;
+
+import java.util.Arrays;
+import java.util.HashMap;
+
+import static junit.framework.Assert.assertEquals;
+
+public class ParquetResultListener implements UserResultsListener {
+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ParquetResultListener.class);
+
+  private SettableFuture<Void> future = SettableFuture.create();
+  int count = 0;
+  int totalRecords;
+  RecordBatchLoader batchLoader;
+  boolean testValues;
+
+  int batchCounter = 1;
+  HashMap<String, Integer> valuesChecked = new HashMap();
+  ParquetTestProperties props;
+
+  ParquetResultListener(RecordBatchLoader batchLoader, ParquetTestProperties props, int numberOfTimesRead, boolean testValues){
+    this.batchLoader = batchLoader;
+    this.props = props;
+    this.totalRecords = props.recordsPerRowGroup * props.numberRowGroups * numberOfTimesRead;
+    this.testValues = testValues;
+  }
+
+  @Override
+  public void submissionFailed(RpcException ex) {
+    logger.debug("Submission failed.", ex);
+    future.setException(ex);
+  }
+
+
+  private <T> void assertField(ValueVector valueVector, int index, TypeProtos.MinorType expectedMinorType, Object value, String name) {
+    assertField(valueVector, index, expectedMinorType, value, name, 0);
+  }
+
+  @SuppressWarnings("unchecked")
+  private <T> void assertField(ValueVector valueVector, int index, TypeProtos.MinorType expectedMinorType, T value, String name, int parentFieldId) {
+
+    if (expectedMinorType == TypeProtos.MinorType.MAP) {
+      return;
+    }
+
+    T val = (T) valueVector.getAccessor().getObject(index);
+    if (val instanceof byte[]) {
+      assertEquals(true, Arrays.equals((byte[]) value, (byte[]) val));
+    } else {
+      assertEquals(value, val);
+    }
+  }
+
+  @Override
+  public void resultArrived(QueryResultBatch result) {
+    logger.debug("result arrived in test batch listener.");
+    if(result.getHeader().getIsLastChunk()){
+      future.set(null);
+    }
+    int columnValCounter = 0;
+    FieldInfo currentField;
+    count += result.getHeader().getRowCount();
+    boolean schemaChanged = false;
+    try {
+      schemaChanged = batchLoader.load(result.getHeader().getDef(), result.getData());
+    } catch (SchemaChangeException e) {
+      throw new RuntimeException(e);
+    }
+
+    int recordCount = 0;
+    // print headers.
+    if (schemaChanged) {
+    } // do not believe any change is needed for when the schema changes, with the current mock scan use case
+
+    for (VectorWrapper vw : batchLoader) {
+      ValueVector vv = vw.getValueVector();
+      currentField = props.fields.get(vv.getField().getName());
+      if (ParquetRecordReaderTest.VERBOSE_DEBUG){
+        System.out.println("\n" + (String) currentField.name);
+      }
+      if ( ! valuesChecked.containsKey(vv.getField().getName())){
+        valuesChecked.put(vv.getField().getName(), 0);
+        columnValCounter = 0;
+      } else {
+        columnValCounter = valuesChecked.get(vv.getField().getName());
+      }
+      for (int j = 0; j < vv.getAccessor().getValueCount(); j++) {
+        if (ParquetRecordReaderTest.VERBOSE_DEBUG){
+          if (vv.getAccessor().getObject(j) instanceof byte[]){
+            System.out.print("[len:" + ((byte[]) vv.getAccessor().getObject(j)).length + " - (");
+            for (int k = 0; k <  ((byte[]) vv.getAccessor().getObject(j)).length; k++){
+              System.out.print((char)((byte[])vv.getAccessor().getObject(j))[k] + ",");
+            }
+            System.out.print(") ]");
+          }
+          else{
+            System.out.print(Strings.padStart(vv.getAccessor().getObject(j) + "", 20, ' ') + " ");
+          }
+          System.out.print(", " + (j % 25 == 0 ? "\n batch:" + batchCounter + " v:" + j + " - " : ""));
+        }
+        if (testValues){
+          assertField(vv, j, currentField.type,
+              currentField.values[columnValCounter % 3], currentField.name + "/");
+        }
+        columnValCounter++;
+      }
+      if (ParquetRecordReaderTest.VERBOSE_DEBUG){
+        System.out.println("\n" + vv.getAccessor().getValueCount());
+      }
+      valuesChecked.remove(vv.getField().getName());
+      valuesChecked.put(vv.getField().getName(), columnValCounter);
+    }
+
+    if (ParquetRecordReaderTest.VERBOSE_DEBUG){
+      for (int i = 0; i < batchLoader.getRecordCount(); i++) {
+        recordCount++;
+        if (i % 50 == 0){
+          System.out.println();
+          for (VectorWrapper vw : batchLoader) {
+            ValueVector v = vw.getValueVector();
+            System.out.print(Strings.padStart(v.getField().getName(), 20, ' ') + " ");
+
+          }
+          System.out.println();
+          System.out.println();
+        }
+
+        for (VectorWrapper vw : batchLoader) {
+          ValueVector v = vw.getValueVector();
+          if (v.getAccessor().getObject(i) instanceof byte[]){
+            System.out.print("[len:" + ((byte[]) v.getAccessor().getObject(i)).length + " - (");
+            for (int j = 0; j <  ((byte[]) v.getAccessor().getObject(i)).length; j++){
+              System.out.print(((byte[])v.getAccessor().getObject(i))[j] + ",");
+            }
+            System.out.print(") ]");
+          }
+          else{
+            System.out.print(Strings.padStart(v.getAccessor().getObject(i) + "", 20, ' ') + " ");
+          }
+        }
+        System.out.println(
+
+        );
+      }
+    }
+    batchCounter++;
+    if(result.getHeader().getIsLastChunk()){
+      // ensure the right number of columns was returned, especially important to ensure selective column read is working
+      assert valuesChecked.keySet().size() == props.fields.keySet().size() : "Unexpected number of output columns from parquet scan,";
+      for (String s : valuesChecked.keySet()) {
+        try {
+          assertEquals("Record count incorrect for column: " + s, totalRecords, (long) valuesChecked.get(s));
+        } catch (AssertionError e) { submissionFailed(new RpcException(e)); }
+      }
+
+      assert valuesChecked.keySet().size() > 0;
+      future.set(null);
+    }
+  }
+
+  public void getResults() throws RpcException{
+    try{
+      future.get();
+    }catch(Throwable t){
+      throw RpcException.mapException(t);
+    }
+  }
+
+  @Override
+  public void queryIdArrived(UserBitShared.QueryId queryId) {
+  }
+}
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetTestProperties.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetTestProperties.java
new file mode 100644
index 0000000..7c68a16
--- /dev/null
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetTestProperties.java
@@ -0,0 +1,37 @@
+/*******************************************************************************
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ******************************************************************************/
+package org.apache.drill.exec.store.parquet;
+
+import java.util.HashMap;
+
+public class ParquetTestProperties {
+
+  int numberRowGroups;
+  int recordsPerRowGroup;
+  int bytesPerPage = 1024 * 1024 * 1;
+  HashMap<String, FieldInfo> fields = new HashMap<>();
+
+  public ParquetTestProperties(int numberRowGroups, int recordsPerRowGroup, int bytesPerPage,
+                               HashMap<String, FieldInfo> fields){
+    this.numberRowGroups = numberRowGroups;
+    this.recordsPerRowGroup = recordsPerRowGroup;
+    this.bytesPerPage = bytesPerPage;
+    this.fields = fields;
+
+  }
+}
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestFileGenerator.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestFileGenerator.java
index d2c95fe..817b7e8 100644
--- a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestFileGenerator.java
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestFileGenerator.java
@@ -24,6 +24,7 @@ import java.util.Map;
 
 import org.apache.drill.common.types.TypeProtos;
 import org.apache.drill.exec.store.ByteArrayUtil;
+import org.apache.drill.exec.store.json.JsonSchemaProvider;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -58,84 +59,60 @@ public class TestFileGenerator {
   static final Object[] binVals = { varLen1, varLen2, varLen3 };
   static final Object[] bin2Vals = { varLen3, varLen2, varLen1 };
 
-  static class FieldInfo {
-
-    String parquetType;
-    String name;
-    int bitLength;
-    int numberOfPages;
-    Object[] values;
-    TypeProtos.MinorType type;
-
-    FieldInfo(int recordsPerRowGroup, String parquetType, String name, int bitLength, Object[] values, TypeProtos.MinorType type) {
-      this.parquetType = parquetType;
-      this.name = name;
-      this.bitLength = bitLength;
-      this.numberOfPages = Math.max(1, (int) Math.ceil( ((long) recordsPerRowGroup) * bitLength / 8.0 / bytesPerPage));
-      this.values = values;
-      // generator is designed to use 3 values
-      assert values.length == 3;
-      this.type = type;
-    }
+  static void populateFieldInfoMap(ParquetTestProperties props){
+    props.fields.put("integer", new FieldInfo("int32", "integer", 32, intVals, TypeProtos.MinorType.INT, props));
+    props.fields.put("bigInt", new FieldInfo("int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT, props));
+    props.fields.put("f", new FieldInfo("float", "f", 32, floatVals, TypeProtos.MinorType.FLOAT4, props));
+    props.fields.put("d", new FieldInfo("double", "d", 64, doubleVals, TypeProtos.MinorType.FLOAT8, props));
+    props.fields.put("b", new FieldInfo("boolean", "b", 1, boolVals, TypeProtos.MinorType.BIT, props));
+    props.fields.put("bin", new FieldInfo("binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY, props));
+    props.fields.put("bin2", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
   }
 
-  private static class WrapAroundCounter {
-
-    int maxVal;
-    int val;
-
-    public WrapAroundCounter(int maxVal) {
-      this.maxVal = maxVal;
-    }
-
-    public int increment() {
-      val++;
-      if (val > maxVal) {
-        val = 0;
-      }
-      return val;
-    }
-
-    public void reset() {
-      val = 0;
-    }
-
+  static void populatePigTPCHCustomerFields(ParquetTestProperties props){
+    // all of the data in the fieldInfo constructors doesn't matter because the file is generated outside the test
+    props.fields.put("C_CUSTKEY", new FieldInfo("int32", "integer", 32, intVals, TypeProtos.MinorType.INT, props));
+    props.fields.put("C_NATIONKEY", new FieldInfo("int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT, props));
+    props.fields.put("C_ACCTBAL", new FieldInfo("float", "f", 32, floatVals, TypeProtos.MinorType.FLOAT4, props));
+    props.fields.put("C_NAME", new FieldInfo("double", "d", 64, doubleVals, TypeProtos.MinorType.FLOAT8, props));
+    props.fields.put("C_ADDRESS", new FieldInfo("boolean", "b", 1, boolVals, TypeProtos.MinorType.BIT, props));
+    props.fields.put("C_PHONE", new FieldInfo("binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY, props));
+    props.fields.put("C_MKTSEGMENT", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
+    props.fields.put("C_COMMENT", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
   }
 
-  public static HashMap<String, FieldInfo> getFieldMap(int recordsPerRowGroup) {
-    HashMap<String, FieldInfo> fields = new HashMap<>();
-    fields.put("integer", new FieldInfo(recordsPerRowGroup, "int32", "integer", 32, intVals, TypeProtos.MinorType.INT));
-    fields.put("bigInt", new FieldInfo(recordsPerRowGroup, "int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT));
-    fields.put("f", new FieldInfo(recordsPerRowGroup, "float", "f", 32, floatVals, TypeProtos.MinorType.FLOAT4));
-    fields.put("d", new FieldInfo(recordsPerRowGroup, "double", "d", 64, doubleVals, TypeProtos.MinorType.FLOAT8));
-    fields.put("b", new FieldInfo(recordsPerRowGroup, "boolean", "b", 1, boolVals, TypeProtos.MinorType.BIT));
-    fields.put("bin", new FieldInfo(recordsPerRowGroup, "binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY));
-    fields.put("bin2", new FieldInfo(recordsPerRowGroup, "binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY));
-    return fields;
+  static void populatePigTPCHSupplierFields(ParquetTestProperties props){
+    // all of the data in the fieldInfo constructors doesn't matter because the file is generated outside the test
+    props.fields.put("S_SUPPKEY", new FieldInfo("int32", "integer", 32, intVals, TypeProtos.MinorType.INT, props));
+    props.fields.put("S_NATIONKEY", new FieldInfo("int64", "bigInt", 64, longVals, TypeProtos.MinorType.BIGINT, props));
+    props.fields.put("S_ACCTBAL", new FieldInfo("float", "f", 32, floatVals, TypeProtos.MinorType.FLOAT4, props));
+    props.fields.put("S_NAME", new FieldInfo("double", "d", 64, doubleVals, TypeProtos.MinorType.FLOAT8, props));
+    props.fields.put("S_ADDRESS", new FieldInfo("boolean", "b", 1, boolVals, TypeProtos.MinorType.BIT, props));
+    props.fields.put("S_PHONE", new FieldInfo("binary", "bin", -1, binVals, TypeProtos.MinorType.VARBINARY, props));
+    props.fields.put("S_COMMENT", new FieldInfo("binary", "bin2", -1, bin2Vals, TypeProtos.MinorType.VARBINARY, props));
   }
 
-  public static void generateParquetFile(String filename, int numberRowGroups, int recordsPerRowGroup) throws Exception {
-    final Map<String, FieldInfo> fields = getFieldMap(recordsPerRowGroup);
+  public static void generateParquetFile(String filename, ParquetTestProperties props) throws Exception {
 
     int currentBooleanByte = 0;
     WrapAroundCounter booleanBitCounter = new WrapAroundCounter(7);
-    
+
     Configuration configuration = new Configuration();
-    configuration.set(ParquetSchemaProvider.HADOOP_DEFAULT_NAME, "file:///");
-    // "message m { required int32 integer; required int64 integer64; required boolean b; required float f; required double d;}"
+    configuration.set(JsonSchemaProvider.HADOOP_DEFAULT_NAME, "file:///");
+    //"message m { required int32 integer; required int64 integer64; required boolean b; required float f; required double d;}"
 
     FileSystem fs = FileSystem.get(configuration);
     Path path = new Path(filename);
-    if (fs.exists(path))
-      fs.delete(path, false);
+    if (fs.exists(path)) fs.delete(path, false);
+
 
     String messageSchema = "message m {";
-    for (FieldInfo fieldInfo : fields.values()) {
+    for (FieldInfo fieldInfo : props.fields.values()) {
       messageSchema += " required " + fieldInfo.parquetType + " " + fieldInfo.name + ";";
     }
     // remove the last semicolon, java really needs a join method for strings...
     // TODO - nvm apparently it requires a semicolon after every field decl, might want to file a bug
-    // messageSchema = messageSchema.substring(schemaType, messageSchema.length() - 1);
+    //messageSchema = messageSchema.substring(schemaType, messageSchema.length() - 1);
     messageSchema += "}";
 
     MessageType schema = MessageTypeParser.parseMessageType(messageSchema);
@@ -145,25 +122,25 @@ public class TestFileGenerator {
     w.start();
     HashMap<String, Integer> columnValuesWritten = new HashMap();
     int valsWritten;
-    for (int k = 0; k < numberRowGroups; k++) {
+    for (int k = 0; k < props.numberRowGroups; k++){
       w.startBlock(1);
       currentBooleanByte = 0;
       booleanBitCounter.reset();
 
-      for (FieldInfo fieldInfo : fields.values()) {
+      for (FieldInfo fieldInfo : props.fields.values()) {
 
-        if (!columnValuesWritten.containsKey(fieldInfo.name)) {
+        if ( ! columnValuesWritten.containsKey(fieldInfo.name)){
           columnValuesWritten.put((String) fieldInfo.name, 0);
           valsWritten = 0;
         } else {
           valsWritten = columnValuesWritten.get(fieldInfo.name);
         }
 
-        String[] path1 = { (String) fieldInfo.name };
+        String[] path1 = {(String) fieldInfo.name};
         ColumnDescriptor c1 = schema.getColumnDescription(path1);
 
-        w.startColumn(c1, recordsPerRowGroup, codec);
-        int valsPerPage = (int) Math.ceil(recordsPerRowGroup / (float) fieldInfo.numberOfPages);
+        w.startColumn(c1, props.recordsPerRowGroup, codec);
+        int valsPerPage = (int) Math.ceil(props.recordsPerRowGroup / (float) fieldInfo.numberOfPages);
         byte[] bytes;
         // for variable length binary fields
         int bytesNeededToEncodeLength = 4;
@@ -171,21 +148,18 @@ public class TestFileGenerator {
           bytes = new byte[(int) Math.ceil(valsPerPage * (int) fieldInfo.bitLength / 8.0)];
         } else {
           // the twelve at the end is to account for storing a 4 byte length with each value
-          int totalValLength = ((byte[]) fieldInfo.values[0]).length + ((byte[]) fieldInfo.values[1]).length
-              + ((byte[]) fieldInfo.values[2]).length + 3 * bytesNeededToEncodeLength;
+          int totalValLength = ((byte[]) fieldInfo.values[0]).length + ((byte[]) fieldInfo.values[1]).length + ((byte[]) fieldInfo.values[2]).length + 3 * bytesNeededToEncodeLength;
           // used for the case where there is a number of values in this row group that is not divisible by 3
           int leftOverBytes = 0;
-          if (valsPerPage % 3 > 0)
-            leftOverBytes += ((byte[]) fieldInfo.values[1]).length + 4;
-          if (valsPerPage % 3 > 1)
-            leftOverBytes += ((byte[]) fieldInfo.values[2]).length + 4;
+          if ( valsPerPage % 3 > 0 ) leftOverBytes += ((byte[])fieldInfo.values[1]).length + bytesNeededToEncodeLength;
+          if ( valsPerPage % 3 > 1 ) leftOverBytes += ((byte[])fieldInfo.values[2]).length + bytesNeededToEncodeLength;
           bytes = new byte[valsPerPage / 3 * totalValLength + leftOverBytes];
         }
         int bytesPerPage = (int) (valsPerPage * ((int) fieldInfo.bitLength / 8.0));
         int bytesWritten = 0;
         for (int z = 0; z < (int) fieldInfo.numberOfPages; z++, bytesWritten = 0) {
           for (int i = 0; i < valsPerPage; i++) {
-            // System.out.print(i + ", " + (i % 25 == 0 ? "\n gen " + fieldInfo.name + ": " : ""));
+            //System.out.print(i + ", " + (i % 25 == 0 ? "\n gen " + fieldInfo.name + ": " : ""));
             if (fieldInfo.values[0] instanceof Boolean) {
 
               bytes[currentBooleanByte] |= bitFields[booleanBitCounter.val]
@@ -195,25 +169,23 @@ public class TestFileGenerator {
                 currentBooleanByte++;
               }
               valsWritten++;
-              if (currentBooleanByte > bytesPerPage)
-                break;
+              if (currentBooleanByte > bytesPerPage) break;
             } else {
-              if (fieldInfo.values[valsWritten % 3] instanceof byte[]) {
-                System.arraycopy(ByteArrayUtil.toByta(((byte[]) fieldInfo.values[valsWritten % 3]).length), 0, bytes,
-                    bytesWritten, bytesNeededToEncodeLength);
-                System.arraycopy(fieldInfo.values[valsWritten % 3], 0, bytes, bytesWritten + bytesNeededToEncodeLength,
-                    ((byte[]) fieldInfo.values[valsWritten % 3]).length);
-                bytesWritten += ((byte[]) fieldInfo.values[valsWritten % 3]).length + bytesNeededToEncodeLength;
-              } else {
-                System.arraycopy(ByteArrayUtil.toByta(fieldInfo.values[valsWritten % 3]), 0, bytes, i
-                    * ((int) fieldInfo.bitLength / 8), (int) fieldInfo.bitLength / 8);
+              if (fieldInfo.values[valsWritten % 3] instanceof byte[]){
+                System.arraycopy(ByteArrayUtil.toByta(((byte[])fieldInfo.values[valsWritten % 3]).length),
+                    0, bytes, bytesWritten, bytesNeededToEncodeLength);
+                System.arraycopy(fieldInfo.values[valsWritten % 3],
+                    0, bytes, bytesWritten + bytesNeededToEncodeLength, ((byte[])fieldInfo.values[valsWritten % 3]).length);
+                bytesWritten += ((byte[])fieldInfo.values[valsWritten % 3]).length + bytesNeededToEncodeLength;
+              } else{
+                System.arraycopy( ByteArrayUtil.toByta(fieldInfo.values[valsWritten % 3]),
+                    0, bytes, i * ((int) fieldInfo.bitLength / 8), (int) fieldInfo.bitLength / 8);
               }
               valsWritten++;
             }
 
           }
-          w.writeDataPage((int) (recordsPerRowGroup / (int) fieldInfo.numberOfPages), bytes.length,
-              BytesInput.from(bytes), PLAIN, PLAIN, PLAIN);
+          w.writeDataPage((int) (props.recordsPerRowGroup / (int) fieldInfo.numberOfPages), bytes.length, BytesInput.from(bytes), PLAIN, PLAIN, PLAIN);
           currentBooleanByte = 0;
         }
         w.endColumn();
diff --git a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/WrapAroundCounter.java b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/WrapAroundCounter.java
new file mode 100644
index 0000000..eef8560
--- /dev/null
+++ b/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/WrapAroundCounter.java
@@ -0,0 +1,40 @@
+/*******************************************************************************
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ******************************************************************************/
+package org.apache.drill.exec.store.parquet;
+
+public class WrapAroundCounter {
+
+  int maxVal;
+  int val;
+
+  public WrapAroundCounter(int maxVal) {
+    this.maxVal = maxVal;
+  }
+
+  public int increment() {
+    val++;
+    if (val > maxVal) {
+      val = 0;
+    }
+    return val;
+  }
+
+  public void reset() {
+    val = 0;
+  }
+}
-- 
1.7.12.4 (Apple Git-37)

